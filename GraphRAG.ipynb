{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105a2123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#   Graph RAG, Vector RAG, Hybrid RAG\n",
    "\n",
    "To enable Cognitive intelligence App towards private data, RAG + LLM and Knowledge Graph are state-of-the-art approaches.\n",
    "\n",
    "In this demo, we will explain know-how of four types of approaches and compare the trade-off and performance among them.\n",
    "\n",
    "| QueryEngine | Graph RAG query engine                                       | Vector RAG query engine                                      | Hybrid RAG query engine                                                                                          |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |------------------------------------------------------------------------------------------------------------------|\n",
    "| Mechanism   | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "\n",
    "## Background, RAG\n",
    "\n",
    "Below digrams are showing how RAG works:\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In VectorDB based RAG, we create embeddings of each node(chunk), and find TopK related ones towards a given question during the query. In the above diagram, nodes `3` and `96` were fetched as the TopK related nodes, used to help answer the user query. \n",
    "\n",
    "## Background Graph RAG\n",
    "\n",
    "In Graph RAG, we will extract relationships between entities, representing concise facts from each node. It would look something like this:\n",
    "\n",
    "```\n",
    "Node Split and Embedding\n",
    "\n",
    "┌────┬────┬────┬────┐\n",
    "│ 1  │ 2  │ 3  │ 4  │\n",
    "├────┴────┴────┴────┤\n",
    "│  Docs/Knowledge   │\n",
    "│        ...        │\n",
    "├────┬────┬────┬────┤\n",
    "│ 95 │ 96 │    │    │\n",
    "└────┴────┴────┴────┘\n",
    "```\n",
    "\n",
    "Then, if we zoomed in of it:\n",
    "\n",
    "```\n",
    "       Node Split and Embedding, with Knowledge Graph being extracted\n",
    "\n",
    "┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
    "│ .─.       .─.    │  .─.       .─.   │            .─.   │  .─.       .─.   │\n",
    "│( x )─────▶ y )   │ ( x )─────▶ a )  │           ( j )  │ ( m )◀────( x )  │\n",
    "│ `▲'       `─'    │  `─'       `─'   │            `─'   │  `─'       `─'   │\n",
    "│  │     1         │        2         │        3    │    │        4         │\n",
    "│ .─.              │                  │            .▼.   │                  │\n",
    "│( z )─────────────┼──────────────────┼──────────▶( i )─┐│                  │\n",
    "│ `◀────┐          │                  │            `─'  ││                  │\n",
    "├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤\n",
    "│       │                      Docs/Knowledge           │                   │\n",
    "│       │                            ...                │                   │\n",
    "│       │                                               │                   │\n",
    "├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤\n",
    "│  .─.  └──────.   │  .─.             │                 ││  .─.             │\n",
    "│ ( x ◀─────( b )  │ ( x )            │                 └┼▶( n )            │\n",
    "│  `─'       `─'   │  `─'             │                  │  `─'             │\n",
    "│        95   │    │   │    96        │                  │   │    98        │\n",
    "│            .▼.   │  .▼.             │                  │   ▼              │\n",
    "│           ( c )  │ ( d )            │                  │  .─.             │\n",
    "│            `─'   │  `─'             │                  │ ( x )            │\n",
    "└──────────────────┴──────────────────┴──────────────────┴──`─'─────────────┘\n",
    "```\n",
    "\n",
    "Where, knowledge, the more granular spliting and information with higher density, optionally multi-hop of `x -> y`, `i -> j -> z -> x` etc... across many more nodes(chunks) than K(in TopK search) could be inlucded in Retrievers. And we believe there are cases that this additional work matters.\n",
    "\n",
    "But how/how well exactly does it work? Let's see in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75004d1",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061a39e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T05:24:03.951307Z",
     "start_time": "2024-08-12T05:23:59.827438Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuxi.zheng/TiHub/inframan/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/_p/nbm6_l1d1c9g0kg6tb6c63dw0000gn/T/ipykernel_74825/2188378047.py:35: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "# For OpenAI\n",
    "import os\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import set_global_service_context\n",
    "\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = OpenAIEmbedding(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_llm,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17442e",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for Local Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ca9fa",
   "metadata": {},
   "source": [
    "❗Access local Graph Store Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cf0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nebula3-python in ./.venv/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: ipython-ngql in ./.venv/lib/python3.9/site-packages (0.14.0)\n",
      "Requirement already satisfied: future>=0.18.0 in ./.venv/lib/python3.9/site-packages (from nebula3-python) (1.0.0)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in ./.venv/lib/python3.9/site-packages (from nebula3-python) (0.22.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in ./.venv/lib/python3.9/site-packages (from nebula3-python) (2024.1)\n",
      "Requirement already satisfied: six>=1.16.0 in ./.venv/lib/python3.9/site-packages (from nebula3-python) (1.16.0)\n",
      "Requirement already satisfied: httpx[http2]>=0.22.0 in ./.venv/lib/python3.9/site-packages (from nebula3-python) (0.27.0)\n",
      "Requirement already satisfied: Jinja2 in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (3.1.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (2.2.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (4.66.1)\n",
      "Requirement already satisfied: pyvis in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (0.3.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (2.31.0)\n",
      "Requirement already satisfied: pydantic in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (2.8.2)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (1.13.1)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (8.1.3)\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.9/site-packages (from ipython-ngql) (17.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./.venv/lib/python3.9/site-packages (from httplib2>=0.20.0->nebula3-python) (3.1.2)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx[http2]>=0.22.0->nebula3-python) (4.4.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx[http2]>=0.22.0->nebula3-python) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx[http2]>=0.22.0->nebula3-python) (1.0.5)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from httpx[http2]>=0.22.0->nebula3-python) (3.4)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from httpx[http2]>=0.22.0->nebula3-python) (1.3.1)\n",
      "Requirement already satisfied: h2<5,>=3 in ./.venv/lib/python3.9/site-packages (from httpx[http2]>=0.22.0->nebula3-python) (4.1.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx[http2]>=0.22.0->nebula3-python) (0.14.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.9/site-packages (from ipywidgets->ipython-ngql) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.9/site-packages (from ipywidgets->ipython-ngql) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.9/site-packages (from ipywidgets->ipython-ngql) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in ./.venv/lib/python3.9/site-packages (from ipywidgets->ipython-ngql) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in ./.venv/lib/python3.9/site-packages (from ipywidgets->ipython-ngql) (3.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from Jinja2->ipython-ngql) (2.1.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.9/site-packages (from pandas->ipython-ngql) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas->ipython-ngql) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas->ipython-ngql) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic->ipython-ngql) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.9/site-packages (from pydantic->ipython-ngql) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.9/site-packages (from pydantic->ipython-ngql) (4.12.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in ./.venv/lib/python3.9/site-packages (from pyvis->ipython-ngql) (3.2.2)\n",
      "Requirement already satisfied: networkx>=1.11 in ./.venv/lib/python3.9/site-packages (from pyvis->ipython-ngql) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->ipython-ngql) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->ipython-ngql) (2.0.4)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in ./.venv/lib/python3.9/site-packages (from h2<5,>=3->httpx[http2]>=0.22.0->nebula3-python) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in ./.venv/lib/python3.9/site-packages (from h2<5,>=3->httpx[http2]>=0.22.0->nebula3-python) (4.0.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets->ipython-ngql) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->ipython-ngql) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->ipython-ngql) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->ipython-ngql) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipython-ngql) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipython-ngql) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipython-ngql) (0.2.3)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nebula3-python ipython-ngql\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"127.0.0.1:9669\" # assumed we have NebulaGraph installed locally\n",
    "\n",
    "space_name = \"guardians\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5613a",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "\n",
    "In our demo, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb7083",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess Data\n",
    "\n",
    "We will download and preprecess data from:\n",
    "    https://en.wikipedia.org/wiki/Guardians_of_the_Galaxy_Vol._3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c376da93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_p/nbm6_l1d1c9g0kg6tb6c63dw0000gn/T/ipykernel_74825/2931829646.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-readers-wikipedia in ./.venv/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in ./.venv/lib/python3.9/site-packages (from llama-index-readers-wikipedia) (0.10.62)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.10.1)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2024.6.1)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.40.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (4.0.3)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2024.7.24)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.3.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.0.5)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Donald Trump'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21c03b",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f3668",
   "metadata": {},
   "source": [
    "This call will take some time, it'll extract entities and relationships and store them into NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c05437d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3efdc4",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG\n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In Llama Index, this could be done with oneline of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474c2251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b5d689dd7739d",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f4afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb936",
   "metadata": {},
   "source": [
    "## 4. Prepare for different query approaches\n",
    "\n",
    "| QueryEngine | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph RAG with Vector query engine                                                                               |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |------------------------------------------------------------------------------------------------------------------|\n",
    "| Mechanism   | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c590de2",
   "metadata": {},
   "source": [
    "### 4. Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me about x, please │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ Below are knowledge about x │             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             │ Please answer based on them │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01e372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06682474",
   "metadata": {},
   "source": [
    "### 4.3 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37713b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93396d7",
   "metadata": {},
   "source": [
    "### 4.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0ee74",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐┌────┐               │             \n",
    "               │ 3  ││ 96 │ x->y<-z,x->h...                            \n",
    "             │ └────┘└────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9516c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe22b2b",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb2d2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_p/nbm6_l1d1c9g0kg6tb6c63dw0000gn/T/ipykernel_74825/1416513360.py:6: DeprecationWarning: Call to deprecated class KGTableRetriever. (KGTableRetriever is deprecated, it is recommended to use PropertyGraphIndex and associated retrievers instead.) -- Deprecated since version 0.10.53.\n",
      "  kg_retriever = KGTableRetriever(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67a63f",
   "metadata": {},
   "source": [
    "And the query engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4976682",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541608be",
   "metadata": {},
   "source": [
    "## 5. Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bafef8",
   "metadata": {},
   "source": [
    "### 5.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dec5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Donald Trump has been involved in various relationships and actions such as being criticized by the World Health Organization, causing an increased incidence of hate crimes, being alleged of election fraud, filing for Chapter 11 bankruptcy protection, playing different versions of himself, and promoting conspiracy theories.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about Donald Trump.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c800",
   "metadata": {},
   "source": [
    "### 5.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08fa71a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Donald Trump is an American politician, media personality, and businessman who served as the 45th president of the United States from 2017 to 2021. He has a Bachelor of Science in economics from the University of Pennsylvania and is known for his real estate ventures, including building skyscrapers, hotels, casinos, and golf courses. Trump has been involved in numerous legal actions and business bankruptcies. As president, he implemented policies such as a travel ban on citizens from certain countries, diverting military funding for a border wall, and rolling back environmental regulations. Trump has been impeached twice, faced controversies over his handling of the COVID-19 pandemic, and made false statements during his presidency. He continues to be a prominent figure in the Republican Party and has faced legal issues post-presidency, including being found guilty of falsifying business records and indicted on multiple felony counts.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about Donald Trump.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f256d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Aspect           | Graph Result                                                                                                                                                                                                                                      | Vector Result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
       "|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| Background       | Focuses on relationships, criticisms, hate crimes, election fraud, bankruptcy, playing different versions of himself, promoting conspiracy theories                                                                                               | Mentions he is an American politician, media personality, businessman, served as the 45th president of the United States, has a Bachelor of Science in economics, known for real estate ventures, legal actions, business bankruptcies, implemented policies as president, impeached twice, controversies over COVID-19 handling, false statements, prominent figure in Republican Party, legal issues post-presidency |\n",
       "| Achievements     | Does not mention any specific achievements                                                                                                                                                                                                        | Mentions building skyscrapers, hotels, casinos, golf courses                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
       "| Policies         | Does not mention any specific policies                                                                                                                                                                                                            | Mentions policies such as travel ban, diverting military funding for border wall, rolling back environmental regulations                                                                                                                                                                                                                                                                                                                                                                             |\n",
       "| Legal Issues     | Mentions being criticized by WHO, alleged election fraud, filing for bankruptcy protection                                                                                                                                                        | Mentions impeachments, controversies over COVID-19 handling, false statements, legal issues post-presidency                                                                                                                                                                                                                                                                                                                                                                                         |\n",
       "| Post-Presidency  | Does not mention any post-presidency information                                                                                                                                                                                                  | Mentions being a prominent figure in Republican Party, facing legal issues post-presidency, being found guilty of falsifying business records, indicted on multiple felony counts                                                                                                                                                                                                                                                                                                                     |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=\"graph_result, vector_result\",\n",
    "    template=\"\"\"\n",
    "Compare the two QA results on \"Tell me about Donald Trump.\" List the differences between them to help evaluate them. Output in a markdown table.\n",
    "\n",
    "**Result from Graph:**\n",
    "{graph_result}\n",
    "\n",
    "---\n",
    "\n",
    "**Result from Vector:**\n",
    "{vector_result}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_args = {\n",
    "    \"graph_result\": response_graph_rag,\n",
    "    \"vector_result\": response_vector_rag\n",
    "}\n",
    "\n",
    "comparison_result = llm.predict(prompt_template, **prompt_args)\n",
    "\n",
    "# Display the comparison result as Markdown\n",
    "display(Markdown(comparison_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e1b07",
   "metadata": {},
   "source": [
    "### 5.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc59b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Donald Trump was born on June 14, 1946, in Queens, New York City. He attended the private Kew-Forest School and later the New York Military Academy before studying economics at the Wharton School of the University of Pennsylvania. Trump is a businessman, media personality, and politician who served as the 45th president of the United States from 2017 to 2021. He has been involved in various business ventures, including real estate, casinos, and television. Throughout his presidency, Trump implemented policies such as tax cuts, border security measures, and changes to environmental regulations. He faced impeachment twice during his presidency and has been involved in numerous legal actions. Trump continues to be a prominent figure in the Republican Party and is a candidate in the 2024 presidential election.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about Donald Trump.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f82da8",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697e4bc",
   "metadata": {},
   "source": [
    "### 6.1 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbae28",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed1e85e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Knowledge Fact                                      | Result Graph | Result Vector | Result Graph+Vector |\n",
       "|-----------------------------------------------------|--------------|---------------|---------------------|\n",
       "| Born on June 14, 1946 in Queens, New York City      | ✓            | ✓             | ✓                   |\n",
       "| Attended Wharton School of the University of Pennsylvania | ✗            | ✓             | ✓                   |\n",
       "| Bachelor of Science in economics from UPenn         | ✗            | ✓             | ✓                   |\n",
       "| Businessman, media personality, politician          | ✗            | ✓             | ✓                   |\n",
       "| 45th president of the United States (2017-2021)     | ✗            | ✓             | ✓                   |\n",
       "| Involved in various business ventures               | ✗            | ✗             | ✓                   |\n",
       "| Implemented policies like tax cuts, border security | ✗            | ✗             | ✓                   |\n",
       "| Faced impeachment twice during presidency           | ✗            | ✗             | ✓                   |\n",
       "| Involved in numerous legal actions and bankruptcies | ✓            | ✓             | ✓                   |\n",
       "| Prominent figure in the Republican Party            | ✗            | ✓             | ✓                   |\n",
       "| Candidate in the 2024 presidential election         | ✗            | ✗             | ✓                   |\n",
       "| Controversies over handling of COVID-19 pandemic    | ✗            | ✗             | ✓                   |\n",
       "| Made false statements during presidency             | ✗            | ✗             | ✓                   |\n",
       "| Legal issues post-presidency                        | ✗            | ✓             | ✓                   |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.prompts.prompts import PromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=\"response_graph_rag, response_vector_rag, response_graph_vector_rag\",\n",
    "    template=\"\"\"\n",
    "Compare the QA results on \"Tell me about Donald Trump.\", list the knowledge facts between them, to help evaluate them. Output in markdown table.\n",
    "\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_args = {\n",
    "    \"response_graph_rag\": response_graph_rag,\n",
    "    \"response_vector_rag\": response_vector_rag,\n",
    "    \"response_graph_vector_rag\": response_graph_vector_rag\n",
    "}\n",
    "\n",
    "comparison_result = llm.predict(prompt_template, **prompt_args)\n",
    "\n",
    "display(Markdown(comparison_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f2ff6-d197-4c7e-88c8-a538384909b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "For the text scenario( need LLM extract to build a KG):\n",
    "\n",
    "- The information loss during the extraction of building KG could impact the GraphRAG peformance a lot \n",
    "- The Graph RAG comes with **concise** results, and much **lower cost** (for cost comparison see  result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results))\n",
    "- The Vector RAG can return fruitful answer, maybe less comprehensive.\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question involves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "| QueryEngine | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c5ca2-e7b1-421b-a1e1-ba3fa9f33042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:47:02.515690Z",
     "start_time": "2024-08-08T09:47:02.512009Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7a074-948c-4f60-a0c8-20f7e99a1350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d10806-ec8b-40dc-bc58-28d436dc8c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba842c52-cffd-4884-937f-d02ed0e6ee14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93e5cc-d988-403b-9c7c-d8b81d88b9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc291e8-e29a-4ccd-b61c-3b45222b6c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a0b09-2161-4120-84e8-01168653d119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a928007-9353-4303-8ca0-96069ae27357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e64420-a0bd-40b8-ab96-cdf50ac46d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570b0eb-f330-425e-a0eb-2bc707c9dcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdedf8-32ce-477a-9bc2-32b7e6fa987c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
